{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "judicial-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.80s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  24598\n",
      "Image Size:  torch.Size([3, 1920, 1080])\n",
      "[{'id': 45, 'image_id': 4, 'category_id': 2, 'bbox': [0.0, 0.0, 1080.0, 1920.0], 'area': 2073600.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 46, 'image_id': 4, 'category_id': 2, 'bbox': [0.0, 63.0, 1080.0, 147.0], 'area': 158760.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 47, 'image_id': 4, 'category_id': 2, 'bbox': [42.0, 101.25, 198.75, 69.75], 'area': 13862.8125, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 48, 'image_id': 4, 'category_id': 1, 'bbox': [849.0, 73.5, 126.0, 126.0], 'area': 15876.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 49, 'image_id': 4, 'category_id': 1, 'bbox': [975.0, 73.5, 105.0, 126.0], 'area': 13230.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 50, 'image_id': 4, 'category_id': 2, 'bbox': [0.0, 1636.5, 1080.0, 10.5], 'area': 11340.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 51, 'image_id': 4, 'category_id': 1, 'bbox': [246.0, 313.5, 735.0, 735.0], 'area': 540225.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 52, 'image_id': 4, 'category_id': 2, 'bbox': [106.5, 1647.0, 63.0, 78.75], 'area': 4961.25, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 53, 'image_id': 4, 'category_id': 2, 'bbox': [60.0, 1725.75, 156.75, 68.25], 'area': 10698.1875, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 54, 'image_id': 4, 'category_id': 2, 'bbox': [374.25, 1647.0, 63.0, 84.0], 'area': 5292.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 55, 'image_id': 4, 'category_id': 2, 'bbox': [360.75, 1735.5, 90.0, 54.0], 'area': 4860.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 56, 'image_id': 4, 'category_id': 2, 'bbox': [642.0, 1647.0, 63.0, 84.0], 'area': 5292.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 57, 'image_id': 4, 'category_id': 2, 'bbox': [559.5, 1735.5, 228.75, 54.0], 'area': 12352.5, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 58, 'image_id': 4, 'category_id': 2, 'bbox': [909.75, 1647.0, 63.0, 84.0], 'area': 5292.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 59, 'image_id': 4, 'category_id': 2, 'bbox': [897.75, 1735.5, 87.0, 54.0], 'area': 4698.0, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 60, 'image_id': 4, 'category_id': 2, 'bbox': [42.0, 1188.75, 94.5, 68.25], 'area': 6449.625, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 61, 'image_id': 4, 'category_id': 2, 'bbox': [42.0, 1328.25, 94.5, 68.25], 'area': 6449.625, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 62, 'image_id': 4, 'category_id': 2, 'bbox': [42.0, 1467.75, 94.5, 68.25], 'area': 6449.625, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 63, 'image_id': 4, 'category_id': 2, 'bbox': [819.0, 1186.5, 219.0, 72.75], 'area': 15932.25, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 64, 'image_id': 4, 'category_id': 2, 'bbox': [178.5, 1186.5, 202.5, 72.75], 'area': 14731.875, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 65, 'image_id': 4, 'category_id': 2, 'bbox': [1008.0, 1326.0, 30.0, 72.75], 'area': 2182.5, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 66, 'image_id': 4, 'category_id': 2, 'bbox': [178.5, 1326.0, 182.25, 72.75], 'area': 13258.6875, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 67, 'image_id': 4, 'category_id': 2, 'bbox': [777.0, 1465.5, 261.0, 72.75], 'area': 18987.75, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}, {'id': 68, 'image_id': 4, 'category_id': 2, 'bbox': [178.5, 1465.5, 233.25, 72.75], 'area': 16968.9375, 'iscrowd': 0, 'ignore': 0, 'segmentation': []}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "cap = dset.CocoDetection(\n",
    "    root = 'rico2coco/rico/dataset/combined/',\n",
    "    annFile = 'dataset/ricoco_clickable.json',\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "print('Number of samples: ', len(cap))\n",
    "img, target = cap[3] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "organizational-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(cap,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-scheduling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
